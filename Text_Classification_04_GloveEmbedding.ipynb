{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_04_GloveEmbedding",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1l7peKKJx7BWPk3oH35BDA_-5ld_5AY3I",
      "authorship_tag": "ABX9TyMPsPFZR2w2otFyVEmiVu/i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilkapil/Text-Classification-101/blob/main/Text_Classification_04_GloveEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B0wh2umkr1C"
      },
      "source": [
        "In this Notebook we will demontrate text classification model using Glove embeddings with Neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMC1Nv-hk6_U"
      },
      "source": [
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import sys \r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\r\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.initializers import Constant \r\n",
        "import zipfile\r\n",
        "import os \r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import regex as re \r\n",
        "from gensim.parsing.preprocessing import remove_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAIzPz-amk8D"
      },
      "source": [
        "! wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZxr7pUcpERz"
      },
      "source": [
        "with zipfile.ZipFile('/content/glove.6B.zip', 'r') as zip_ref:\r\n",
        "    zip_ref.extractall('/content/Glove')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIU73Tk9gUhD"
      },
      "source": [
        "train_df = pd.read_csv('/content/train_2kmZucJ.csv')\r\n",
        "test_df = pd.read_csv('/content/test_oJQbWVk.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0QHyqcbgSr0"
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \r\n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \r\n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \r\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \r\n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "  text = str(text)\r\n",
        "  for punc in puncts:\r\n",
        "      if punc in text:\r\n",
        "          text = text.replace(punc, ' ')\r\n",
        "  return text\r\n",
        "\r\n",
        "def remove_emoji(text):\r\n",
        "    emoji_pattern = re.compile(\"[\"\r\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           u\"\\U00002702-\\U000027B0\"\r\n",
        "                           u\"\\U000024C2-\\U0001F251\"\r\n",
        "                           \"]+\", flags=re.UNICODE)\r\n",
        "    return emoji_pattern.sub(r'', text)\r\n",
        "    \r\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x: remove_emoji(x)) \r\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x: clean_text(x)) \r\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x: re.sub(r'http\\S+','',x))\r\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x: re.sub(\"@[\\w]*\", '', x))\r\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x:' '.join(x.split()))\r\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x: remove_stopwords(x))\r\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x: x.lower())\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Ey-1QBg1mb"
      },
      "source": [
        "#Preprocessing the test dataset as well\r\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x: remove_emoji(x)) \r\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x: clean_text(x)) \r\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x: re.sub(r'http\\S+','',x))\r\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x: re.sub(\"@[\\w]*\", '', x))\r\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x:' '.join(x.split()))\r\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x: remove_stopwords(x))\r\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x: x.lower())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E_j-MDSuyvX"
      },
      "source": [
        "GLOVE_DIR = '/content/Glove'\r\n",
        "\r\n",
        "MAX_LENGTH_SEQ = 1000\r\n",
        "MAX_NUM_WORDS = 20000\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "VALIDATION_SPLIT = 0.20\r\n",
        "LABELS_LEN = train_df['label'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCpEwdxZpcTh"
      },
      "source": [
        "X = train_df['tweet']\r\n",
        "y = train_df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFAVUuNtwRFk"
      },
      "source": [
        "## Loading and Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Kfix88wSwc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNz84YqMub_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef7f394-a057-42d2-e8bf-a38c014b1a12"
      },
      "source": [
        "##Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\r\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\r\n",
        "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS )\r\n",
        "tokenizer.fit_on_texts(train_df['tweet'])\r\n",
        "train_sequences =   tokenizer.texts_to_sequences(train_df['tweet'])\r\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df['tweet'])\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 23144 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8NKOuiDvHTO"
      },
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\r\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\r\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_LENGTH_SEQ)\r\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_LENGTH_SEQ)\r\n",
        "train_labels = to_categorical(np.asarray(train_df['label']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB202G0WrnND"
      },
      "source": [
        "#split the training data into a training se and validation set \r\n",
        "indices = np.arange(train_data.shape[0])\r\n",
        "np.random.shuffle(indices)\r\n",
        "train_data = train_data[indices]\r\n",
        "train_labels = train_labels[indices]\r\n",
        "num_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\r\n",
        "x_train = train_data[:-num_validation_samples]\r\n",
        "y_train = train_labels[:-num_validation_samples]\r\n",
        "x_val = train_data[-num_validation_samples:]\r\n",
        "y_val = train_labels[-num_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEIbSwZra1Wv"
      },
      "source": [
        "print('Preparing embedding matrix')\r\n",
        "\r\n",
        "#First build index mapping words in the embedding set\r\n",
        "#to their embedding vector \r\n",
        "\r\n",
        "embedding_index = {}\r\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\r\n",
        "  for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "    embedding_index[word] = coefs\r\n",
        "\r\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embedding_index))\r\n",
        "print(embedding_index[\"google\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-SHtoqcR2o"
      },
      "source": [
        "#Prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from the Glove\r\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\r\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\r\n",
        "print(embedding_matrix.shape)\r\n",
        "for word, i in word_index.items():\r\n",
        "  print(i, word)\r\n",
        "  if i > MAX_NUM_WORDS:\r\n",
        "    continue\r\n",
        "  embedding_vector = embedding_index.get(word)\r\n",
        "  if embedding_vector is not None:\r\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx1gfJ40eEbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75757728-86fc-4f27-d0b1-82811e6ff21e"
      },
      "source": [
        "# load these pre-trained word embeddings into an Embedding layer\r\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\r\n",
        "\r\n",
        "embedding_layer = Embedding(num_words,\r\n",
        "                            EMBEDDING_DIM,\r\n",
        "                            embeddings_initializer = Constant(embedding_matrix),\r\n",
        "                            input_length=MAX_LENGTH_SEQ,\r\n",
        "                            trainable=False)\r\n",
        "print('Preparing of embedding matrix is done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing of embedding matrix is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABTBnitxmug8"
      },
      "source": [
        "\r\n",
        "**1D CNN Model with pre-trained embedding¶** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxqTpq99mwkh"
      },
      "source": [
        "cnnmodel = Sequential()\r\n",
        "cnnmodel.add(embedding_layer)\r\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "cnnmodel.add(MaxPooling1D(5))\r\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "cnnmodel.add(MaxPooling1D(5))\r\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "cnnmodel.add(GlobalMaxPooling1D())\r\n",
        "cnnmodel.add(Dense(128, activation='relu'))\r\n",
        "cnnmodel.add(Dense(LABELS_LEN, activation='softmax'))\r\n",
        "\r\n",
        "cnnmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\r\n",
        "cnnmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5YpYguv9Ght"
      },
      "source": [
        "**LSTM Model with training your own embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9vgcrju9MwO",
        "outputId": "71e42e85-00ef-4419-ae23-983de9a881ab"
      },
      "source": [
        "rnnmodel = Sequential()\r\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\r\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "rnnmodel.add(Dense(2, activation='sigmoid'))\r\n",
        "rnnmodel.compile(loss='binary_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n",
        "print('Training the RNN')\r\n",
        "\r\n",
        "rnnmodel.fit(x_train, y_train,\r\n",
        "          batch_size=32,\r\n",
        "          epochs=1,\r\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Training the RNN\n",
            "198/198 [==============================] - 290s 1s/step - loss: 0.4286 - accuracy: 0.7985 - val_loss: 0.2342 - val_accuracy: 0.9028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8660376668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh3UtuygC8Za"
      },
      "source": [
        "**Bidirectional LSTM with 3 Output Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoxGjIY_DDD8",
        "outputId": "6dc61f98-9c13-44af-f3aa-e725ac59bb1f"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(embedding_layer)\r\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)))\r\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)))\r\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(64)))\r\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='uniform'))\r\n",
        "model.add(Dense(2, activation='sigmoid'))\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data= (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "198/198 [==============================] - 62s 289ms/step - loss: 0.4077 - accuracy: 0.8155 - val_loss: 0.2828 - val_accuracy: 0.8674\n",
            "Epoch 2/5\n",
            "198/198 [==============================] - 55s 280ms/step - loss: 0.2773 - accuracy: 0.8805 - val_loss: 0.2537 - val_accuracy: 0.8946\n",
            "Epoch 3/5\n",
            "198/198 [==============================] - 55s 280ms/step - loss: 0.2718 - accuracy: 0.8859 - val_loss: 0.2528 - val_accuracy: 0.8933\n",
            "Epoch 4/5\n",
            "198/198 [==============================] - 56s 281ms/step - loss: 0.2220 - accuracy: 0.9047 - val_loss: 0.2460 - val_accuracy: 0.8971\n",
            "Epoch 5/5\n",
            "198/198 [==============================] - 56s 285ms/step - loss: 0.2612 - accuracy: 0.8857 - val_loss: 0.3411 - val_accuracy: 0.8712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8524c76588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zSg2ZG9Goqv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}